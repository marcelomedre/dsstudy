submit()
telegram("Good", "morning")
submit()
mad_libs(place = Londrina, adjective = beautiful, noun = city)
mad_libs(place = "Londrina", adjective = "beautiful", noun = "city")
submit()
'I' %p% 'love' %p% 'R!'
x <- rnorm(10)
summary(x)
str(x)
set.seed(20)
x<-rnorm(100)
e<- rnorm(100,0,2)
y <- 0.5 + 2*x+e
summary(y)
plot(x,y)
library(swirl)
swirl()
head(flags)
dim(flags)
class(flags)
cls_list <- lapply(flags, class)
cls_list
class(cls_list)
as.character(cls_list)
cls_list <- sapply(flags, class)
cls_vect <- sapply(flags, class)
class(cls_vect)
sum(flags$orange)
flag_colors <- flags[, 11:17]
head(flag_colors)
lapply(flag_colors, sum)
sapply(flag_colors, sum)
sapply(flag_colors, mean)
flag_shapes <- flags[, 19:23]
lapply(flag_shapes, range)
shape_mat <- sapply(flag_shapes, range)
shape_mat
class(shape_mat)
unique(c(3, 4, 5, 5, 6, 6))
unique(c(3, 4, 5, 5, 5, 6, 6))
unique_vals <- lapply(flags, unique)
unique_vals
sapply(unique_vals, length)
sapply(flags, unique)
lapply(unique_vals, function(elem) elem[2])
set.seed(1)
rpois(5,2)
set.seed(10)
x<-rep(0:1, each =5)
e<-rnorm(10, 0, 20)
y <- 0.5 +2*x+e
plot(x,y)
wd
setwd()
36,92*(1+(1/6))
36.92*(1+(1/6))
43.07*40
1722*4.5
13.63*0.84
220000*0.1144
220000*0.1318
28996*0.85
13.63*0.8
220000*0.10904
13.18*0.85
fileUrl <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType=DOWNLOAD"
download.file(fileUrl, destfile = "./data/cameras.xlsx", method = "curl")
if(!file.exists("data")){dir.create("data")}
fileUrl <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType=DOWNLOAD"
download.file(fileUrl, destfile = "./data/cameras.xlsx", method = "curl")
fileUrl <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType=DOWNLOAD&bom=true"
fileUrl <- "https://data.baltimorecity.gov/api/views/dz54-2aru/rows.tsv?accessType=DOWNLOAD"
download.file(fileUrl, destfile = "./data/cameras.xlsx", method = "curl")
download.file(fileUrl, destfile = "./data/cameras.xlsx", method = "auto")
list.files("./data")
dateDownloaded <- date()
dateDownloaded
head(cameraData)
install.packages("xlxs")
install.packages("xlsx")
library(xlsx)
library(xlsx)
library(xlsx)
library(xlsx)
install.packages("rJava")
install.packages("rJava")
library(xlsx)
install_from_swirl(Getting and Cleaning data
install_from_swirl(Getting and Cleaning data)
library(swirl)
install_from_swirl(Getting and Cleaning data)
install_from_swirl(Getting And Cleaning Data)
install.packages("data.table")
library(data.table)
DF <- data.frame(x=rnorm(9), y=rep(c("a", "b", "C"), each = 3), znorm(9))
DF <- data.frame(x=rnorm(9), y=rep(c("a", "b", "C"), each = 3), z=norm(9))
DF = data.frame(x=rnorm(9), y=rep(c("a", "b", "C"), each = 3), z=norm(9))
DF = data.frame(x=rnorm(9), y=rep(c("a", "b", "C"), each = 3), z=norm(9))
DF <- data.frame(x=rnorm(9), y=rep(c("a", "b", "C"), each = 3), z=rnorm(9))
head(DF, 3)
DT <- data.table(x=rnorm(9), y=rep(c("a", "b", "C"), each = 3), z=rnorm(9))
head(DT, 3)
tables()
DT[2,]
DT[c(2,3)]
getwd()
getwd()
if(!file.exists("data")){
dir.create("data")
}
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(fileUrl,destfile = "./data/week1quiz.csv", method = "curl")
download.file(fileUrl,destfile = "./data/week1quiz.csv", method = "auto")
houseData <- read.csv("week1quiz.csv")
houseData <- read.csv("./data/week1quiz.csv")
head(houseData)
val <- houseData$VAL
head(val)
sum(val >= 24)
length(val[val >= 24])
length(val[val == 24])
length(!is.na(val[val == 24])
length(!is.na(val[val == 24]))
length(houseData$VAL[!is.na(houseData$VAL) & houseData$VAL == 24])
fileUrl2 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx"
download.file(fileUrl2, destfile = "./data/NatGasAquiProg.xlsx", method = "libcurl")
list.files("./data")
library(xlsx)
NGAData <- read.xlsx("./data/NatGasAquiProg.xlsx", sheetIndex= 1,header=TRUE)
download.file(fileUrl2, destfile = "./data/NatGasAquiProg.xlsx", method = "wb")
list.files("./data")
library(xlsx)
NGAData <- read.xlsx("./data/NatGasAquiProg.xlsx", sheetIndex= 1,header=TRUE)
download.file(fileUrl2, destfile = "./data/NatGasAquiProg.xlsx", mode = "w", method = "curl")
download.file(url=fileUrl2, destfile="gov_NGAP.xlsx", mode="w", method="curl")
download.file(fileUrl2, destfile = "./data/NatGasAquiProg.xlsx", mode = "wb", method = "curl")
download.file(fileUrl2, destfile = "./data/NatGasAquiProg.xlsx", mode = "wb", method = "curl")
download.file(fileUrl2, destfile = "./data/NatGasAquiProg.xlsx", mode = "wb", method = "auto")
NGAData <- read.xlsx("./data/NatGasAquiProg.xlsx", sheetIndex= 1,header=TRUE)
dateDownloaded <- date()
dateDownloaded
colIndex <- 7:15
rowIndex <- 18:23
dat <- read.xlsx("./data/NatGasAquiProg.xlsx", sheetIndex= 1,
colIndex = colIndex,
rowIndex = rowIndex,
header=TRUE)
sum(dat$Zip*dat$Ext, na.rm = TRUE)
library(XML)
fileUrl3 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
BaltimoreRestaurants <- xmlTreeParse(fileUrl3, useInternal = TRUE)
BaltimoreRestaurants <- xmlTreeParse(fileUrl3, useInternal = TRUE)
doc <- xmlTreeParse(file=fileUrl3,useInternal=TRUE)
xData <- getURL(fileUrl3)
install.packages("RCurl")
library(RCurl)
install.packages("RCurl")
library(RCurl)
fileUrl3 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
xData <- getURL(fileUrl3)
BaltimoreRestaurants <- xmlTreeParse(fileUrl3, useInternal = TRUE)
library(XML)
BaltimoreRestaurants <- xmlTreeParse(fileUrl3, useInternal = TRUE)
fileUrl3 <- "http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
BaltimoreRestaurants <- xmlTreeParse(fileUrl3, useInternal = TRUE)
rootNode(BaltimoreRestaurants)
rootNode <- xmlRoot(BaltimoreRestaurants)
rootNode
rootNode(BaltimoreRestaurants)
xmlName(rootNode)
zipcode <- xmlSApply(rootNode, "//zipcode" xmlValue)
zipcode <- xmlSApply(rootNode, "//zipcode", xmlValue)
zipcode <- xpathSApply(rootNode, "//zipcode", xmlValue)
length(zipcode[zipcode == 21231])
library(data.table)
fileUrl4 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(url=fileUrl4, destfile="Housing_Idaho.csv", mode="w", method="curl")
download.file(url=fileUrl4, destfile="Housing_Idaho.csv", mode="wb", method="curl")
download.file(fileUrl4, destfile="./data/Housing_Idaho.csv", method="auto")
?fread
DT <- fread(input = "Housing_Idaho.csv", sep = ",")
DT <- fread(input = "./data/Housing_Idaho.csv", sep = ",")
pwgtp15 <- system.time(mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15))
pwgtp15 <- system.time(mean(DT[DT$SEX==1,]$pwgtp15)
pwgtp15 <- system.time(mean(DT[DT$SEX==1,]$pwgtp15))
system.time(DT[,mean(pwgtp15)], by = SEX)
system.time(DT[,mean(pwgtp15), by = SEX])
system.time(mean(DT[DT$SEX==1,]$pwgtp15))
system.timemean(DT$pwgtp15,by=DT$SEX))
system.timemean(DT$pwgtp15,by=DT$SEX)
system.time(mean(DT$pwgtp15,by=DT$SEX))
system.time(rowMeans(DT)[DT$SEX==1])
system.time(DT[,mean(pwgtp15),by=SEX])
system.time(tapply(DT$pwgtp15,DT$SEX,mean))
system.time(sapply(split(DT$pwgtp15,DT$SEX),mean))
0.3/100
3453*0.003
2656*0.003
111/6
library(swirl)
packageVersion(swirl)
packageVersion("swirl")
install_from_swirl("Getting and Cleaning Data")
swirl()
library(dplyr)
cran <- tbl_df(mydf)
rm("mydf")
cran
?group_by
by_package <- group_by(cran, package)
by_package
summarize(mean(by_package))
?summarize
summarize(by_package, mean(size))
?n
?n_unique
?n_distinct
submit()
pack_sum
quantile(pack_sum$count, probs = 0.99)
top_counts <- filter(pack_sum[pack_sum$count > 679])
?filter
?filter()
top_counts <- filter(pack_sum, count > 679])
top_counts <- filter(pack_sum, count > 679)
top_counts
View(top_counts)
?arrange
top_counts_sorted <- arrange(top_counts, desc(count))
View(top_counts_sorted)
quantile(pack_sum$unique, probs probs = 0.99)
quantile(pack_sum$unique, probs = 0.99)
top_unique <- filter(pack_sum, unique > 465)
View(top_unique)
arrange(top_unique, unique)
arrange(top_unique, desc(unique)
)
arrange(top_unique, desc(unique))
top_unique_sorted <- arrange(top_unique, desc(unique))
View(top_unique_sorted)
aubmit()
submit()
submit()
submit()
View(result3)
submit
submit()
submit
submit()
submit()
?mutate
submit()
submit()
submit()
submit()
submit()
submit()
1.007
1.007/70000
ans <- 1.07/70000
ans*20
1.07*20
21.4/70000
0.0003*100
install.packages("RMySQL", type = "source")
library(RMySQL)
ucscDb <- dbConnect(MySQL(), user = "genome",
host = "genome-mysql.cse.ucsc.edu")
result <- dbGetQuery(ucscDb, "Show databases;"); dbDisconnect(ucscDb)
result
hg19 <- dbConnect(MySQL(), user = "genome", db = "hg19",
host = "genome-mysql.cse.ucsc.edu")
allTables <- dbListTables(hg19)
length(allTables)
allTables[1:5]
dbListFields(hg19, "affyU133Plus2")
dbGetQuery(hg19, "select count(*) from affyU133Plus2")
affData <- dbReadTable(hg19, "affyU133Plus2")
head(affData)
query <- dbSendQuery(hg19, "select * from affyU133Plus2 where mismatches between 1 and 3")
affMis <- fetch(query); quantile(affMis$misMatches)
affMisSmall <- fetch(query, n=10); dbClearResult(query);
dim(affMisSmall)
dbDisconnect(hg19)
library(httr)
require(httpuv)
install.packages("httpuv")
require(httpuv)
oauth_endpoints("github")
myapp <- oauth_app("quiz2",
key = "7bb89f22156737b7958a",
secret = "2f8db11bd573309cac5c0435c75d0a80a6c0ccd8")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
req <- GET("https://api.github.com/users/jtleek/repos", config(token = github_token))
repo_list <- content(req)
repo_list
library(jsonlite)
json1 = repo_list
json2 = jsonlite::fromJSON(toJSON(json1))
View(json2)
View(json2)
View(json2)
xpathSApply(repo_list, "//datasharing", xmlValue)
repo <- c()
for (i in length(repo_list)){
repo <- repo_list[i]
if (repo$name == "datasharing"){
our_repo = repo
break
}
}
repo_list
head(repo_list)
list(repo_list$names, repo_list$created_at)
repo <- c()
for (i in length(repo_list)){
repo <- repo_list[i]
if (repo$name == "datasharing"){
our_repo = repo
break
}
}
for (i in 1:length(repo_list)){
repo <- repo_list[i]
if (repo$name == "datasharing"){
our_repo = repo
break
}
}
answer1 <- c()
for (i in 1:length(repo_list)) {
repo <- repo_list[[i]]
if (repo$name == "datasharing") {
answer1 = repo
break
}
}
if (length(answer1) == 0) {
msg("No such repository found: 'datasharing'")
} else {
msg("The repository 'datasharing' was created at", answer1$created_at)
}
repo <- c()
for (i in 1:length(repo_list)){
repo <- repo_list[[i]]
if (repo$name == "datasharing"){
our_repo = repo
break
}
}
our_repo
?msg
?message
if(length(our_repo) == 0){
message("No such repository found :'datasharing'")
}else {
message("The repository 'datasharing' was created at", our_repo$created_at)
}
install.packages("sqldf")
library(sqldf)
library(RMySQL)
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(url, destfile = "acs.csv", method = "auto")
survey <- read.csv("acs.csv")
survey <- read.csv("acs.csv", header = TRUE)
head(survey)
sqldf("Select pwgtp1 from acs where AGEP < 50")
acs <- read.csv("acs.csv", header = TRUE)
query 1 <- sqldf("Select pwgtp1 from acs where AGEP < 50")
query1 <- sqldf("Select pwgtp1 from acs where AGEP < 50")
query1 <- sqldf("Select pwgtp1 from acs where AGEP < 50", acs)
query1 <- sqldf("Select pwgtp1 from acs where AGEP < 50", drv = "acs")
acs <- read.csv("acs.csv", sep =  ",", header = TRUE)
query1 <- sqldf("Select pwgtp1 from acs where AGEP < 50", drv = "acs")
query1 <- sqldf("Select pwgtp1 from acs where AGEP < 50")
?sqldf
query1 <- sqldf("Select pwgtp1 from acs where AGEP < 50", drv = "RMySQL")
View(acs)
sqldf("Select pwgtp1 from acs where AGEP < 50")
fname <- "survey.csv"
download_if_not_exists(fname, "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv")
download.file(fname, "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv")
acs <- read.csv("acs.csv", header = TRUE, sep = ",")
answer2 <- sqldf("select pwgtp1 from acs where AGEP < 50")
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
f <- file.path(getwd(), "ss06pid.csv")
download.file(url, f)
acs <- data.table(read.csv(f))
acs <- read.csv(f)
query1 <- sqldf("select pwgtp1 from acs where AGEP < 50")
library(sqldf)
library(RMySQL)
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(url, destfile = "acs.csv", method = "auto")
acs <- read.csv("acs.csv", sep =  ",", header = TRUE)
sqldf("Select pwgtp1 from acs where AGEP < 50", drv = "SQLite")
query1 <- sqldf("Select pwgtp1 from acs where AGEP < 50", drv = "SQLite")
View(query1)
query2 <- sqldf("select pwgtp1 from acs", drv = "SQLite")
query3 <- sqldf("select * from acs where AGEP < 50", drv = "SQLite")
query4 <- sqldf("select * from acs", drv = "SQLite")
View(query2)
View(query3)
answer <- unique(acs$AGEP)
answer <- unique(acs$AGEP)
?identical
query1 <- sqldf("select unique AGEP from acs", drv = "SQLite")
query2 <- sqldf("select distinct AGEP from acs", drv = "SQLite")
query3 <- sqldf("select AGEP where unique from acs", drv = "SQLite")
query4 <- sqldf("select distinct pwgtp1 from acs", drv = "SQLite")
identical(answer, query1)
identical(answer, query2)
identical(answer, query3)
identical(answer, query4)
query1 <- sqldf("select unique AGEP from acs")
View(query2)
library(XML)
library(httr)
?nchar
library(XML)
library(httr)
con <- url("http://biostat.jhsph.edu/~jleek/contact.html")
html <- readLines(con)
close(con)
answer <- c(nchar(html[10]), nchar(html[20]), nchar(html[30]), nchar(html[100]))
answer
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for"
download.file(url, destfile = "data.for", method = "auto")
data <- read.fwf(data.for)
?read.fwf
lines <- readLines(url, n=10)
widths <- c(1, 9, 5, 4, 1, 3, 5, 4, 1, 3, 5, 4, 1, 3, 5, 4, 1, 3)
colNames <- c("filler", "week", "filler", "sstNino12", "filler",
"sstaNino12", "filler", "sstNino3", "filler", "sstaNino3",
"filler", "sstNino34", "filler", "sstaNino34", "filler",
"sstNino4", "filler", "sstaNino4")
data <- read.fwf(data.for, widths = widths, header = FALSE, skip = 4, col.names = colNames)
data <- read.fwf(url, widths = widths, header = FALSE, skip = 4, col.names = colNames)
data <- read.fwf("data.for", widths = widths, header = FALSE, skip = 4, col.names = colNames)
View(data)
answer <- sum(data[,4])
answer
View(data)
data <- read.fwf("data.for", widths = widths, header = FALSE, col.names = colNames)
View(data)
data <- read.fwf("data.for", widths = widths, header = FALSE, skip = 4, col.names = colNames)
?grep
d <- d[, grep("^[^filler]", names(d))]
d <- d[, grep("^[^filler]", names(data))]
data <- data[, grep("^[^filler]", names(data))]
sum(d[, 4])
sum(data[, 4])
View(data)
answer <- sum(data[,4])
answer
getwd()
setwd("C:/Users/Marcelo/Desktop/Data/Study/Titanic")
train <- read.csv("train.csv")
test <- read.csv("test.csv")
library(rpart)
library(randomForest)
library(party)
test$Survived <- NA
combi <- rbind(train, test)
combi$Name <- as.character(combi$Name)
combi$Title <- sapply(combi$Name, FUN=function(x) {strsplit(x, split='[,.]')[[1]][2]})
combi$Title <- sub(' ', '', combi$Title)
combi$Title[combi$Title %in% c('Mme', 'Mlle')] <- 'Mlle'
combi$Title[combi$Title %in% c('Capt', 'Don', 'Major', 'Sir')] <- 'Sir'
combi$Title[combi$Title %in% c('Dona', 'Lady', 'the Countess', 'Jonkheer')] <- 'Lady'
combi$Title <- factor(combi$Title)
combi$FamilySize <- combi$SibSp + combi$Parch + 1
combi$Surname <- sapply(combi$Name, FUN=function(x) {strsplit(x, split='[,.]')[[1]][1]})
combi$FamilyID <- paste(as.character(combi$FamilySize), combi$Surname, sep="")
combi$FamilyID[combi$FamilySize <= 2] <- 'Small'
famIDs <- data.frame(table(combi$FamilyID))
famIDs <- famIDs[famIDs$Freq <= 2,]
combi$FamilyID[combi$FamilyID %in% famIDs$Var1] <- 'Small'
combi$FamilyID <- factor(combi$FamilyID)
summary(combi$Age)
Agefit <- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + Title + FamilySize,
data=combi[!is.na(combi$Age),], method="anova")
combi$Age[is.na(combi$Age)] <- predict(Agefit, combi[is.na(combi$Age),])
summary(combi)
summary(combi$Embarked)
which(combi$Embarked == '')
combi$Embarked[c(62,830)] = "S"
combi$Embarked <- factor(combi$Embarked)
summary(combi$Fare)
which(is.na(combi$Fare))
combi$Fare[1044] <- median(combi$Fare, na.rm=TRUE)
combi$FamilyID2 <- combi$FamilyID
combi$FamilyID2 <- as.character(combi$FamilyID2)
combi$FamilyID2[combi$FamilySize <= 3] <- 'Small'
combi$FamilyID2 <- factor(combi$FamilyID2)
train <- combi[1:891,]
test <- combi[892:1309,]
set.seed(415)
fit <- randomForest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title + FamilySize + FamilyID2,
data=train, importance=TRUE, ntree=2000)
varImpPlot(fit)
Prediction <- predict(fit, test)
submit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
write.csv(submit, file = "firstforest.csv", row.names = FALSE)
set.seed(415)
fit <- cforest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title + FamilySize + FamilyID,
data = train, controls=cforest_unbiased(ntree=2000, mtry=3))
Prediction <- predict(fit, test, OOB=TRUE, type = "response")
submit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
write.csv(submit, file = "ciforest.csv", row.names = FALSE)
